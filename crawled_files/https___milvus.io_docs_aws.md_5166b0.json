{"content": "<iframe src=\"https://www.googletagmanager.com/ns.html?id=G-V1G3KQ048M\" height=\"0\" width=\"0\" style=\"display: none; visibility: hidden\" aria-hidden=\"true\"></iframe>DocsTutorialsToolsBlogCommunityTry Managed Milvus FREEDocsTutorialsToolsBlogCommunityStars22kJoin SlackTry Managed Milvus FREEHow we use cookiesThis website stores cookies on your computer. By continuing to browse or by clicking \u2018Accept\u2019, you agree to the storing of cookies on your device to enhance your site experience and for analytical purposes.AcceptSearch\u2318KHomev2.2.x\u200bAbout MilvusGet StartedUser GuideAdministration GuideConfigure MilvusManage DependenciesAllocate ResourcesDeploy on CloudsAWSAmazon EC2Amazon EKSGCPAzureScale a Milvus ClusterManage Resource GroupsUpgradeMonitor and AlertSecurityIntegrationsBenchmarksToolsReferenceExample ApplicationsFAQsAPI referenceDeploy a Milvus Cluster on EC2\nThis topic describes how to deploy a Milvus cluster on Amazon EC2 with Terraform and Ansible.\nProvision a Milvus cluster\nThis section describes how to use Terraform to provision a Milvus cluster.\nTerraform is an infrastructure as code (IaC) software tool. With Terraform, you can provision infrastructure by using declarative configuration files.\nPrerequisites\n\n\nInstall and configure Terraform\n\n\nInstall and configure AWS CLI\n\n\nPrepare configuration\nYou can download template configuration files at Google Drive.\n\n\nmain.tf\nThis file contains the configuration for provisioning a Milvus cluster.\n\n\nvariables.tf\nThis file allows quick editing of variables used to set up or update a Milvus cluster.\n\n\noutput.tf and inventory.tmpl\nThese files store the metadata of a Milvus cluster. The metadata used in this topic is the public_ip for each node instance, private_ip for each node instance, and all EC2 instance IDs.\n\n\nPrepare variables.tf\nThis section describes the configuration that a variables.tf file that contains.\n\n\nNumber of nodes\nThe following template declares an index_count variable used to set the number of index nodes.\nThe value of index_count must be greater than or equal to one.\nvariable \"index_count\" {\n  description = \"Amount of index instances to run\"\n  type        = number\n  default     = 5\n}\n\n\n\nInstance type for a node type\nThe following template declares an index_ec2_type variable used to set the instance type for index nodes.\nvariable \"index_ec2_type\" {\n  description = \"Which server type\"\n  type        = string\n  default     = \"c5.2xlarge\"\n}\n\n\n\nAccess permission\nThe following template declares a key_name variable and a my_ip variable. The key_name variable represents the AWS access key. The my_ip variable represents the IP address range for a security group.\nvariable \"key_name\" {\n  description = \"Which aws key to use for access into instances, needs to be uploaded already\"\n  type        = string\n  default     = \"\"\n}\n\nvariable \"my_ip\" {\n  description = \"my_ip for security group. used so that ansible and terraform can ssh in\"\n  type        = string\n  default     = \"x.x.x.x/32\"\n}\n\n\n\nPrepare main.tf\nThis section describes the configurations that a main.tf file that contains.\n\n\nCloud provider and region\nThe following template uses the us-east-2 region. See Available Regions for more information.\nprovider \"aws\" {\n  profile = \"default\"\n  region  = \"us-east-2\"\n}\n\n\n\nSecurity group\nThe following template declares a security group that allows incoming traffic from the CIDR address range represented by my_ip declared in variables.tf.\nresource \"aws_security_group\" \"cluster_sg\" {\n  name        = \"cluster_sg\"\n  description = \"Allows only me to access\"\n  vpc_id      = aws_vpc.cluster_vpc.id\n\n  ingress {\n    description      = \"All ports from my IP\"\n    from_port        = 0\n    to_port          = 65535\n    protocol         = \"tcp\"\n    cidr_blocks      = [var.my_ip]\n  }\n\n  ingress {\n    description      = \"Full subnet communication\"\n    from_port        = 0\n    to_port          = 65535\n    protocol         = \"all\"\n    self             = true\n  }\n\n  egress {\n    from_port        = 0\n    to_port          = 0\n    protocol         = \"-1\"\n    cidr_blocks      = [\"0.0.0.0/0\"]\n    ipv6_cidr_blocks = [\"::/0\"]\n  }\n\n  tags = {\n    Name = \"cluster_sg\"\n  }\n}\n\n\n\nVPC\nThe following template specifies a VPC with the 10.0.0.0/24 CIDR block on a Milvus cluster.\nresource \"aws_vpc\" \"cluster_vpc\" {\n  cidr_block = \"10.0.0.0/24\"\n  tags = {\n    Name = \"cluster_vpc\"\n  }\n}\n\nresource \"aws_internet_gateway\" \"cluster_gateway\" {\n  vpc_id = aws_vpc.cluster_vpc.id\n\n  tags = {\n    Name = \"cluster_gateway\"\n  }\n}\n\n\n\nSubnets (Optional)\nThe following template declares a subnet whose traffic is routed to an internet gateway. In this case, the size of the subnet's CIDR block is the same as the VPC's CIDR block.\nresource \"aws_subnet\" \"cluster_subnet\" {\n  vpc_id                  = aws_vpc.cluster_vpc.id\n  cidr_block              = \"10.0.0.0/24\"\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = \"cluster_subnet\"\n  }\n}\n\nresource \"aws_route_table\" \"cluster_subnet_gateway_route\" {\n  vpc_id       = aws_vpc.cluster_vpc.id\n\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.cluster_gateway.id\n  }\n\n  tags = {\n    Name = \"cluster_subnet_gateway_route\"\n  }\n}\n\nresource \"aws_route_table_association\" \"cluster_subnet_add_gateway\" {\n  subnet_id      = aws_subnet.cluster_subnet.id\n  route_table_id = aws_route_table.cluster_subnet_gateway_route.id\n}\n\n\n\n\nNode instances (Nodes)\nThe following template declares a MinIO node instance. The main.tf template file declares nodes of 11 node types. For some node types, you need to set root_block_device. See EBS, Ephemeral, and Root Block Devices for more information.\nresource \"aws_instance\" \"minio_node\" {\n  count         = var.minio_count\n  ami           = \"ami-0d8d212151031f51c\"\n  instance_type = var.minio_ec2_type\n  key_name      = var.key_name\n  subnet_id     = aws_subnet.cluster_subnet.id \n  vpc_security_group_ids = [aws_security_group.cluster_sg.id]\n\n  root_block_device {\n    volume_type = \"gp2\"\n    volume_size = 1000\n  }\n  \n  tags = {\n    Name = \"minio-${count.index + 1}\"\n  }\n}\n\n\n\nApply the configuration\n\n\nOpen a terminal and navigate to the folder that stores main.tf.\n\n\nTo initialize the configuration, run terraform init.\n\n\nTo apply the configuration, run terraform apply and enter yes when prompted.\n\n\nYou have now provisioned a Milvus cluster with Terraform.\nStart the Milvus cluster\nThis section describes how to use Ansible to start the Milvus cluster that you have provisioned.\nAnsible is a configuration management tool used to automate cloud provisioning and configuration management.\nPrerequisites\n\nAnsible Controller is installed.\n\nDownload Ansible Milvus node deployment Playbook\nClone Milvus repository from GitHub to download the Ansible Milvus node deployment Playbook.\ngit clone https://github.com/milvus-io/milvus.git\n\nConfigure installation files\nThe inventory.ini and ansible.cfg files are used to control the environment variables and log-in verification methods in Ansible playbook. In the inventory.ini file, the dockernodes section defines all the servers of docker engines. The ansible.cfg section defines all the servers of Milvus coordinators. The node section defines all the servers of Milvus nodes.\nEnter the local path to the Playbook and configure the installation files.\n$ cd ./milvus/deployments/docker/cluster-distributed-deployment\n\ninventory.ini\nConfigure inventory.ini to divide hosts in groups in accordance with their roles in the Milvus system.\nAdd host names, and define docker group and vars.\n[dockernodes] #Add docker host names.\ndockernode01\ndockernode02\ndockernode03\n\n[admin] #Add Ansible controller name.\nansible-controller\n\n[coords] #Add the host names of Milvus coordinators.\n; Take note the IP of this host VM, and replace 10.170.0.17 with it.\ndockernode01\n\n[nodes] #Add the host names of Milvus nodes.\ndockernode02\n\n[dependencies] #Add the host names of Milvus dependencies.\n; dependencies node will host etcd, minio, pulsar, these 3 roles are the foundation of Milvus. \n; Take note the IP of this host VM, and replace 10.170.0.19 with it.\ndockernode03\n\n[docker:children]\ndockernodes\ncoords\nnodes\ndependencies\n\n[docker:vars]\nansible_python_interpreter= /usr/bin/python3\nStrictHostKeyChecking= no\n\n; Setup variables to controll what type of network to use when creating containers.\ndependencies_network= host\nnodes_network= host\n\n; Setup varibale to controll what version of Milvus image to use.\nimage= milvusdb/milvus-dev:master-20220412-4781db8a\n\n; Setup static IP addresses of the docker hosts as variable for container environment variable config.\n; Before running the playbook, below 4 IP addresses need to be replaced with the IP of your host VM\n; on which the etcd, minio, pulsar, coordinators will be hosted.\netcd_ip= 10.170.0.19\nminio_ip= 10.170.0.19\npulsar_ip= 10.170.0.19\ncoords_ip= 10.170.0.17\n\n; Setup container environment which later will be used in container creation.\nETCD_ENDPOINTS= {{etcd_ip}}:2379 \nMINIO_ADDRESS= {{minio_ip}}:9000\nPULSAR_ADDRESS= pulsar://{{pulsar_ip}}:6650\nQUERY_COORD_ADDRESS= {{coords_ip}}:19531\nDATA_COORD_ADDRESS= {{coords_ip}}:13333\nROOT_COORD_ADDRESS= {{coords_ip}}:53100\nINDEX_COORD_ADDRESS= {{coords_ip}}:31000\n\nansible.cfg\nansible.cfg controls the action of the playbook, for example, SSH key, etc. Do not set up passphrase via the SSH key on docker hosts. Otherwise, the Ansible SSH connection will fail. We recommend setting up the same username and SSH key on the three hosts and setting up the new user account to execute sudo without a password. Otherwise, you will receive errors that the user name does not match the password or you are not granted elevated privileges when running Ansible playbook.\n[defaults]\nhost_key_checking = False\ninventory = inventory.ini # Specify the Inventory file\nprivate_key_file=~/.my_ssh_keys/gpc_sshkey # Specify the SSH key that Ansible uses to access Docker host\n\ndeploy-docker.yml\ndeploy-docker.yml defines the tasks during the installation of Docker. See the code comments in the file for details.\n---\n- name: setup pre-requisites # Install prerequisite\n  hosts: all\n  become: yes\n  become_user: root\n  roles:\n    - install-modules\n    - configure-hosts-file\n\n- name: install docker\n  become: yes\n  become_user: root\n  hosts: dockernodes\n  roles:\n    - docker-installation\n\nTest Ansible connectivity\nTest the connectivity to Ansible.\n$ ansible all -m ping\n\nAdd -i in the command to specify the path to the inventory file if you did not specify it in ansible.cfg, otherwise Ansible uses /etc/ansible/hosts.\nThe terminal returns as follow:\ndockernode01 | SUCCESS => {\n\"changed\": false,\n\"ping\": \"pong\"\n}\nansible-controller | SUCCESS => {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\ndockernode03 | SUCCESS => {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\ndockernode02 | SUCCESS => {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n\nCheck the Playbook Syntax\nCheck the syntax of the Playbook.\n$ ansible-playbook deploy-docker.yml --syntax-check\n\nNormally, the terminal returns as follow:\nplaybook: deploy-docker.yml\n\nInstall Docker\nInstall Docker with the Playbook.\n$ ansible-playbook deploy-docker.yml\n\nIf Docker is successfully installed on the three hosts, the terminal returns as follow:\nTASK [docker-installation : Install Docker-CE] *******************************************************************\nok: [dockernode01]\nok: [dockernode03]\nok: [dockernode02]\n\nTASK [docker-installation : Install python3-docker] **************************************************************\nok: [dockernode01]\nok: [dockernode02]\nok: [dockernode03]\n\nTASK [docker-installation : Install docker-compose python3 library] **********************************************\nchanged: [dockernode01]\nchanged: [dockernode03]\nchanged: [dockernode02]\n\nPLAY RECAP *******************************************************************************************************\nansible-controller         : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\ndockernode01               : ok=10   changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\ndockernode02               : ok=10   changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\ndockernode03               : ok=10   changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\nVerify the installation\nLog in to the three hosts with the SSH key, and verify the installation on the hosts.\n\nFor root host:\n\n$ docker -v\n\n\nFor non-root hosts:\n\n$ sudo docker -v\n\nNormally, the terminal returns as follow:\nDocker version 20.10.14, build a224086\n\nCheck the running status of the containers.\n$ docker ps\n\nCheck the Syntax\nCheck the Syntax of deploy-milvus.yml.\n$ ansible-playbook deploy-milvus.yml --syntax-check\n\nNormally, the terminal returns as follow:\nplaybook: deploy-milvus.yml\n\nCreate Milvus container\nThe tasks to create Milvus container are defined in deploy-milvus.yml.\n$ ansible-playbook deploy-milvus.yml\n\nThe terminal returns:\nPLAY [Create milvus-etcd, minio, pulsar] *****************************************************************\n\nTASK [Gathering Facts] ********************************************************************************************\nok: [dockernode03]\n\nTASK [etcd] *******************************************************************************************************\nchanged: [dockernode03]\n\nTASK [pulsar] *****************************************************************************************************\nchanged: [dockernode03]\n\nTASK [minio] ******************************************************************************************************\nchanged: [dockernode03]\n\nPLAY [Create milvus nodes] ****************************************************************************************\n\nTASK [Gathering Facts] ********************************************************************************************\nok: [dockernode02]\n\nTASK [querynode] **************************************************************************************************\nchanged: [dockernode02]\n\nTASK [datanode] ***************************************************************************************************\nchanged: [dockernode02]\n\nTASK [indexnode] **************************************************************************************************\nchanged: [dockernode02]\n\nPLAY [Create milvus coords] ***************************************************************************************\n\nTASK [Gathering Facts] ********************************************************************************************\nok: [dockernode01]\n\nTASK [rootcoord] **************************************************************************************************\nchanged: [dockernode01]\n\nTASK [datacoord] **************************************************************************************************\nchanged: [dockernode01]\n\nTASK [querycoord] *************************************************************************************************\nchanged: [dockernode01]\n\nTASK [indexcoord] *************************************************************************************************\nchanged: [dockernode01]\n\nTASK [proxy] ******************************************************************************************************\nchanged: [dockernode01]\n\nPLAY RECAP ********************************************************************************************************\ndockernode01               : ok=6    changed=5    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\ndockernode02               : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\ndockernode03               : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\nNow you have Milvus deployed on the three hosts.\nStop nodes\nYou can stop all nodes after you do not need a Milvus cluster any longer.\n Ensure that the terraform binary is available on your PATH. \n\n\nRun terraform destroy and enter yes when prompted.\n\n\nIf successful, all node instances are stopped.\n\n\nWhat's next\nIf you want to learn how to deploy Milvus on other clouds:\n\nDeploy a Milvus Cluster on EKS\nDeploy Milvus Cluster on GCP with Kubernetes\nGuide to Deploying Milvus on Microsoft Azure With Kubernetes\naws.md was last updated at 2022-11-23 23:24:00: Fix broken links in 2.2.x docs_1124 (#1874)Edit this pageReport a bugRequest doc changesOn this pageDeploy a Milvus Cluster on EC2Provision a Milvus clusterStart the Milvus clusterStop nodesWhat's nextResourcesDocsBlogManaged serviceTutorialsBootcampDemoVideoToolsAttuMilvus CLISizing ToolMivlus backup ToolCommunityGet involvedSlackGithubForumMilvus. 2023 All rights reserved./*<![CDATA[*/window.pagePath=\"/docs/aws.md\";/*]]>*//*<![CDATA[*/window.___chunkMapping={\"polyfill\":[\"/polyfill.js\"],\"app\":[\"/app.js\"],\"component---src-pages-404-js\":[\"/component---src-pages-404-js.js\",\"/component---src-pages-404-js.ffef32ceb6972d1b6709.css\"],\"component---src-pages-bootcamp-jsx\":[\"/component---src-pages-bootcamp-jsx.js\",\"/component---src-pages-bootcamp-jsx.edf1a55d3c49094e9b41.css\"],\"component---src-pages-community-index-jsx\":[\"/component---src-pages-community-index-jsx.js\",\"/component---src-pages-community-index-jsx.c505d2a5fff7c1e4b2c8.css\"],\"component---src-pages-index-jsx\":[\"/component---src-pages-index-jsx.js\",\"/component---src-pages-index-jsx.45df0d4bdfe2db469d6c.css\"],\"component---src-pages-milvus-demos-index-jsx\":[\"/component---src-pages-milvus-demos-index-jsx.js\",\"/component---src-pages-milvus-demos-index-jsx.1a3d1c4caf417de255ae.css\"],\"component---src-pages-milvus-demos-reverse-image-search-jsx\":[\"/component---src-pages-milvus-demos-reverse-image-search-jsx.js\",\"/component---src-pages-milvus-demos-reverse-image-search-jsx.c41e4c5ead83649937e8.css\"],\"component---src-pages-slack-jsx\":[\"/component---src-pages-slack-jsx.js\",\"/component---src-pages-slack-jsx.b3b356986d01fe89f9d1.css\"],\"component---src-pages-tools-sizing-jsx\":[\"/component---src-pages-tools-sizing-jsx.js\",\"/component---src-pages-tools-sizing-jsx.66d1c05015f9508df90b.css\"],\"component---src-templates-api-doc-template-jsx\":[\"/component---src-templates-api-doc-template-jsx.js\",\"/component---src-templates-api-doc-template-jsx.6c57c73b99682695a5ef.css\"],\"component---src-templates-blog-list-template-jsx\":[\"/component---src-templates-blog-list-template-jsx.js\",\"/component---src-templates-blog-list-template-jsx.71f592374d478c258caf.css\"],\"component---src-templates-blog-template-jsx\":[\"/component---src-templates-blog-template-jsx.js\",\"/component---src-templates-blog-template-jsx.5068317f62135173b013.css\"],\"component---src-templates-doc-template-jsx\":[\"/component---src-templates-doc-template-jsx.js\",\"/component---src-templates-doc-template-jsx.0e85fbce360982ff881c.css\"]};/*]]>*/", "content_type": "text", "score": null, "meta": {"url": "https://milvus.io/docs/aws.md"}, "id_hash_keys": ["content"], "embedding": null, "id": "1ee6b2e14b39b8c7d111b041eb80bb18"}