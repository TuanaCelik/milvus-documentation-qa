{"content": "<iframe src=\"https://www.googletagmanager.com/ns.html?id=G-V1G3KQ048M\" height=\"0\" width=\"0\" style=\"display: none; visibility: hidden\" aria-hidden=\"true\"></iframe>DocsTutorialsToolsBlogCommunityTry Managed Milvus FREEDocsTutorialsToolsBlogCommunityStars22kJoin SlackTry Managed Milvus FREEHow we use cookiesThis website stores cookies on your computer. By continuing to browse or by clicking \u2018Accept\u2019, you agree to the storing of cookies on your device to enhance your site experience and for analytical purposes.AcceptSearch\u2318KHomev2.2.x\u200bAbout MilvusGet StartedUser GuideManage Milvus ConnectionsManage DatabasesManage CollectionsManage PartitionsManage DataInsert EntitiesInsert Entities from FilesDelete EntitiesCompact DataManage IndexesSearch and QuerySearch with Time TravelBootcampAdministration GuideIntegrationsBenchmarksToolsReferenceExample ApplicationsFAQsAPI referenceInsert Entities from Files\nMilvus 2.2 now supports inserting a batch of entities from a file. Compared to the insert() method, this feature reduces network transmission across the Milvus client, proxy, Pulsar, and data nodes. You can now import a batch of entities in one file or multiple files into a collection with just a few lines of code.\nPrepare the data file\nOrganize the data to be inserted into a Milvus collection in a row-based JSON file or multiple NumPy files.\nRow-based JSON file\nYou can name the file whatever makes sense, but the root key must be rows. In the file, each entity is organized in a dictionary. The keys in the dictionary are field names, and the values are field values in the corresponding entity.\nThe following is an example of a row-based JSON file. You can include fields not defined in the collection schema as dynamic fields. For details, refer to Dynamic Schema.\n\n{\n  \"rows\":[\n    {\"book_id\": 101, \"word_count\": 13, \"book_intro\": [1.1, 1.2]},\n    {\"book_id\": 102, \"word_count\": 25, \"book_intro\": [2.1, 2.2]},\n    {\"book_id\": 103, \"word_count\": 7, \"book_intro\": [3.1, 3.2]},\n    {\"book_id\": 104, \"word_count\": 12, \"book_intro\": [4.1, 4.2]},\n    {\"book_id\": 105, \"word_count\": 34, \"book_intro\": [5.1, 5.2]}\n  ]\n}\n\n# To include dynamic fields, do as follows:\n\n{\n  \"rows\":[\n    {\"book_id\": 101, \"word_count\": 13, \"book_intro\": [1.1, 1.2], \"book_props\": {\"year\": 2015, \"price\": 23.43}},\n    {\"book_id\": 102, \"word_count\": 25, \"book_intro\": [2.1, 2.2], \"book_props\": {\"year\": 2018, \"price\": 15.05}},\n    {\"book_id\": 103, \"word_count\": 7, \"book_intro\": [3.1, 3.2], \"book_props\": {\"year\": 2020, \"price\": 36.68}},\n    {\"book_id\": 104, \"word_count\": 12, \"book_intro\": [4.1, 4.2] , \"book_props\": {\"year\": 2019, \"price\": 20.14}},\n    {\"book_id\": 105, \"word_count\": 34, \"book_intro\": [5.1, 5.2] , \"book_props\": {\"year\": 2021, \"price\": 9.36}}\n  ]\n}\n\n\n\n\nDo not add any field that does not exist in the target collection, and do not miss any field that the schema of the target collection defines.\nTo add fields that are not predefined in the schema, you should enable dynamic schema for the collection. In this case, Milvus automatically adds these fields to an internal JSON field. For details, refer to Dynamic Schema.\nUse the correct types of values in each field. For example, use integers in integer fields, floats in float fields, strings in varchar fields, and float arrays in vector fields.\nDo not include an auto-generated primary key in the JSON file.\nFor binary vectors, use uint8 arrays. Each uint8 value represents 8 dimensions, and the value must be between 0 and 255. For example, [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1] is a 16-dimensional binary vector and should be written as [128, 7] in the JSON file.\nIf you have enabled dynamic schema for a collection, you can add fields that are not pre-defined in the schema. Milvus automatically adds these non-existent fields into a JSON field.\n\n\nColumn-based NumPy files\nAs an alternative to the row-based JSON file mentioned above, you can also use NumPy arrays to organize each column of a dataset in a separate file. In this case, use the field name of each column to name the NumPy file.\n\nimport numpy\nnumpy.save('book_id.npy', numpy.array([101, 102, 103, 104, 105]))\nnumpy.save('word_count.npy', numpy.array([13, 25, 7, 12, 34]))\narr = numpy.array([[1.1, 1.2],\n            [2.1, 2.2],\n            [3.1, 3.2],\n            [4.1, 4.2],\n            [5.1, 5.2]])\nnumpy.save('book_intro.npy', arr)\narr = numpy.array([json.dumps({\"year\": 2015, \"price\": 23.43}),\n            json.dumps({\"year\": 2018, \"price\": 15.05}),\n            json.dumps({\"year\": 2020, \"price\": 36.68}),\n            json.dumps({\"year\": 2019, \"price\": 20.14}),\n            json.dumps({\"year\": 2021, \"price\": 9.36}))\nnumpy.save('book_props.npy', arr)\n\n\nYou can also add dynamic fields using NumPy files as follows. For details on dynamic fields, refer to Dynamic Schema.\nnumpy.save('$meta.py', numpy.array([ json.dumps({x: 2}), json.dumps({y: 8, z: 2}) ]))\n\n\n\nUse the field name of each column to name the NumPy file. Do not add files named after a field that does not exist in the target collection. There should be one NumPy file for each field.\nUse the correct value type when creating NumPy arrays. For details, refer to these examples.\n\n\nInsert entities from files\n1. Upload data files\nYou can use either MinIO or the local hard disk for storage in Milvus.\n\nUsing the local hard disk for storage is only available in Milvus Standalone.\n\n\nTo use MinIO for storage, upload data files to the bucket defined by minio.bucketName in the milvus.yml configuration file.\nFor local storage, copy the data files into a directory of the local disk.\n\n2. Insert entities\nTo facilitate data import from files, Milvus offers a bulk-insert API in various flavors. In PyMilvus, you can use the do_bulk_insert() method. As to the Java SDK, use the bulkInsert method.\nIn this method, you need to set the name of the target collection as collection_name and the list of files prepared in the previous step as files. Optionally, you can specify the name of a specific partition as partition_name in the target collection so that Milvus imports the data from the files listed only into this partition.\n\n\nFor a row-based JSON file, parameter files should be a one-member list containing the path to the JSON file.\n\n  Python \n  Java\n\nfrom pymilvus import utility\ntask_id = utility.do_bulk_insert(\n    collection_name=\"book\",\n    partition_name=\"2022\",\n    files=[\"test.json\"]\n)\n\nimport io.milvus.param.bulkinsert.BulkInsertParam;\nimport io.milvus.response.BulkInsertResponseWrapper;\nimport io.milvus.grpc.ImportResponse;\nimport io.milvus.param.R;\n\nBulkInsertParam param = BulkInsertParam.newBuilder()\n        .withCollectionName(\"book\")\n        .withPartitionName(\"2022\")\n        .addFile(\"test.json\")\n        .build()\nR<ImportResponse> response = milvusClient.bulkInsert(param);\nBulkInsertResponseWrapper wrapper = new BulkInsertResponseWrapper(response.getData());\ntask_id = wrapper.getTaskID();\n\n\n\nFor a set of column-based NumPy files, parameter files should be a multi-member list containing the paths to the NumPy files.\n\n  Python \n  Java\n\nfrom pymilvus import utility\ntask_id = utility.do_bulk_insert(\n    collection_name=\"book\",\n    partition_name=\"2022\",\n    files=[\"book_id.npy\", \"word_count.npy\", \"book_intro.npy\", \"book_props.npy\"]\n)\n\nimport io.milvus.param.bulkinsert.BulkInsertParam;\nimport io.milvus.response.BulkInsertResponseWrapper;\nimport io.milvus.grpc.ImportResponse;\nimport io.milvus.param.R;\n\nBulkInsertParam param = BulkInsertParam.newBuilder()\n        .withCollectionName(\"book\")\n        .withPartitionName(\"2022\")\n        .addFile(\"book_id.npy\")\n        .addFile(\"word_count.npy\")\n        .addFile(\"book_intro.npy\")\n        .addFile(\"book_props.npy\")\n        .build()\nR<ImportResponse> response = milvusClient.bulkInsert(param);\nBulkInsertResponseWrapper wrapper = new BulkInsertResponseWrapper(response.getData());\ntask_id = wrapper.getTaskID();\n\nEach call to the bulk-insert API returns immediately. The return value is the ID of a data-import task running in the background. Milvus maintains a queue of such tasks to be dispatched in parallel to idle data nodes.\n\nWhen setting the file paths, note that\n\nIf you upload the data file to a MinIO instance, a valid file path should be relative to the root bucket defined in \"milvus.yml\", such as \"data/book_id.npy\".\nIf you upload the data file to the local hard drive, a valid file path should be an absolute path such as \"/tmp/data/book_id.npy\".\n\nIf you have a lot of files to process, consider creating multiple data-import tasks and have them run in parallel.\n\n\n\nList tasks\nCheck task state\nSince the bulk-insert API is asynchronous, you might need to check whether a data-import task is complete. Milvus provides a BulkInsertState object to hold the details of a data-import task and you can use the get-bulk-insert-state API to retrieve this object using the programming language of your choice.\nIn the flavor of PyMilvus, you can use get_bulk_insert_state(). For Java SDK, use getBulkInsertState().\n\n  Python \n  Java\n\ntask = utility.get_bulk_insert_state(task_id=task_id)\nprint(\"Task state:\", task.state_name)\nprint(\"Imported files:\", task.files)\nprint(\"Collection name:\", task.collection_name)\nprint(\"Partition name:\", task.partition_name)\nprint(\"Start time:\", task.create_time_str)\nprint(\"Imported row count:\", task.row_count)\nprint(\"Entities ID array generated by this task:\", task.ids)\n\nif task.state == BulkInsertState.ImportFailed:\n    print(\"Failed reason:\", task.failed_reason)\n\nimport io.milvus.param.bulkinsert.GetBulkInsertStateParam;\nimport io.milvus.response.GetBulkInsertStateWrapper;\nimport io.milvus.grpc.GetImportStateResponse;\nimport io.milvus.grpc.ImportState;\nimport io.milvus.param.R;\n\nGetBulkInsertStateParam param = GetBulkInsertStateParam.newBuilder()\n        .withTask(task_id)\n        .build()\nR<GetImportStateResponse> response = milvusClient.getBulkInsertState(param);\nGetBulkInsertStateWrapper wrapper = new GetBulkInsertStateWrapper(response.getData());\nImportState state = wrapper.getState();\nlong row_count = wrapper.getImportedCount();\nString create_ts = wrapper.getCreateTimeStr();\nString failed_reason = wrapper.getFailedReason();\nString files = wrapper.getFiles();\nint progress = wrapper.getProgress();\n\nThe following table lists the state of a data-import task returned.\n\n\n\nState\nCode\nDescription\n\n\n\n\nPending\n0\nThe task is pending.\n\n\nFailed\n1\nThe task fails. Use task.failed_reason to understand why the task fails.\n\n\nStarted\n2\nThe task has been dispatched to a data node and will be executed soon.\n\n\nPersisted\n5\nNew data segments have been generated and persisted.\n\n\nCompleted\n6\nThe metadata has been updated for the new segments.\n\n\nFailed and cleaned\n7\nThe task fails and all temporary data generated by this task are cleared.\n\n\n\nList all tasks\nMilvus also offers a list-bulk-insert-tasks API for you to list all data-import tasks. In this method, you need to specify a collection name so that Milvus lists all tasks that import data into this collection. Optionally, you can specify a limit for the maximum number of tasks to return.\n\n  Python \n  Java\n\ntasks = utility.list_bulk_insert_tasks(collection_name=\"book\", limit=10)\nfor task in tasks:\n    print(task)\n\nimport io.milvus.param.bulkinsert.ListBulkInsertTasksParam;\nimport io.milvus.grpc. ListImportTasksResponse;\nimport io.milvus.grpc.GetImportStateResponse;\nimport io.milvus.grpc.ImportState;\nimport io.milvus.param.R;\n\nListBulkInsertTasksParam param = ListBulkInsertTasksParam.newBuilder()\n    .withCollectionName(\"book\")\n    .build()\nR<ListImportTasksResponse> response = milvusClient.listBulkInsertTasks(param);\nList<GetImportStateResponse> tasks = response.getTasksList();\nfor (GetImportStateResponse task : tasks) {\n    GetBulkInsertStateWrapper wrapper = new GetBulkInsertStateWrapper(task);\n    ImportState state = wrapper.getState();\n    long row_count = wrapper.getImportedCount();\n    String create_ts = wrapper.getCreateTimeStr();\n    String failed_reason = wrapper.getFailedReason();\n    String files = wrapper.getFiles();\n}\n\n\n\n\nParameter\nDescription\n\n\n\n\ncollection_name (optional)\nSpecify the target collection name to list all tasks on this collection. Leave the value empty if you want to list all tasks recorded by Milvus root coords.\n\n\nlimit (optional)\nSpecify this parameter to limit the number of returned tasks.\n\n\n\nSee System Configurations for more information about import task configurations.\nLimits\n\n\n\nFeature\nMaximum limit\n\n\n\n\nMax. size of task pending list\n65536\n\n\nMax. size of a data file\n16 GB\n\n\n\nReference\nConfigure Milvus for data import\nTo have Milvus remove failed or old data-import tasks automatically, you can specify a timeout duration and retention period for data-import tasks in the Milvus configuration file.\nrootCoord:\n  # (in seconds) Duration after which an import task will expire (be killed). Default 900 seconds (15 minutes).\n  # Note: If default value is to be changed, change also the default in: internal/util/paramtable/component_param.go\n  importTaskExpiration: 900\n  # (in seconds) Milvus will keep the record of import tasks for at least `importTaskRetention` seconds. Default 86400\n  # seconds (24 hours).\n  # Note: If default value is to be changed, change also the default in: internal/util/paramtable/component_param.go\n  importTaskRetention: 86400\n\nCreate NumPy files\nThe following examples demonstrate how to create NumPy files for columns of data types that Milvus supports.\n\n\nCreate a Numpy file from a boolean array\n\nimport numpy as np\ndata = [True, False, True, False]\ndt = np.dtype('bool', (len(data)))\narr = np.array(data, dtype=dt)\nnp.save(file_path, arr)\n\n\n\n\nCreate a NumPy file from an int8 array\n\nimport numpy as np\ndata = [1, 2, 3, 4]\ndt = np.dtype('int8', (len(data)))\narr = np.array(data, dtype=dt)\nnp.save(file_path, arr)\n\n\n\n\nCreate a NumPy file from an int16 array\n\nimport numpy as np\ndata = [1, 2, 3, 4]\ndt = np.dtype('int16', (len(data)))\narr = np.array(data, dtype=dt)\nnp.save(file_path, arr)\n\n\n\n\nCreate a NumPy file from an int32 array\n\nimport numpy as np\ndata = [1, 2, 3, 4]\ndt = np.dtype('int32', (len(data)))\narr = np.array(data, dtype=dt)\nnp.save(file_path, arr)\n\n\n\n\nCreate a NumPy file from an int64 array\n\nimport numpy as np\ndata = [1, 2, 3, 4]\ndt = np.dtype('int64', (len(data)))\narr = np.array(data, dtype=dt)\nnp.save(file_path, arr)\n\n\n\n\nCreate a NumPy file from a float array\n\nimport numpy as np\ndata = [0.1, 0.2, 0.3, 0.4]\ndt = np.dtype('float32', (len(data)))\narr = np.array(data, dtype=dt)\nnp.save(file_path, arr)\n\n\n\n\nCreate a NumPy file from a double float array\n\nimport numpy as np\ndata = [0.1, 0.2, 0.3, 0.4]\ndt = np.dtype('float64', (len(data)))\narr = np.array(data, dtype=dt)\nnp.save(file_path, arr)\n\n\n\n\nCreate a NumPy file from a VARCHAR array\n\ndata = [\"a\", \"b\", \"c\", \"d\"]\narr = np.array(data)\nnp.save(file_path, arr)\n\n\n\n\nCreate a NumPy file from a binary vector array\nFor binary vectors, use uint8 as the NumPy data type. Each uint8 value represents 8 dimensions. For a 32-dimensional binary vector, use four uint8 values.\n\ndata = [\n    [43, 35, 124, 90],\n    [65, 212, 12, 57],\n    [6, 126, 232, 78],\n    [87, 189, 38, 22],\n]\ndt = np.dtype('uint8', (len(data), 4))\narr = np.array(data)\nnp.save(file_path, arr)\n\n\n\n\nCreate a NumPy file from a float vector array\nIn Milvus, you can use either float32 or float64 values to form a float vector.\nThe following snippet creates a NumPy file from an 8-dimensional vector array formed using float32 values.\n\ndata = [\n    [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8],\n    [2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8],\n    [3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8],\n    [4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8],\n]\ndt = np.dtype('float32', (len(data), 8))\narr = np.array(data)\nnp.save(file_path, arr)\n\n\n\n\nImport multiple NumPy files in parallel\nYou can upload NumPy files into different subdirectories, create multiple import tasks, and execute them in parallel.\nAssume the data structure is as follows:\n\u251c\u2500\u2500 task_1\n\u2502    \u2514\u2500\u2500 book_id.npy\n\u2502    \u2514\u2500\u2500 word_count.npy\n\u2502    \u2514\u2500\u2500 book_intro.npy\n\u2502    \u2514\u2500\u2500 book_props.npy\n\u251c\u2500\u2500 task_2\n\u2502    \u2514\u2500\u2500 book_id.npy\n\u2502    \u2514\u2500\u2500 word_count.npy\n\u2502    \u2514\u2500\u2500 book_intro.npy\n\u2502    \u2514\u2500\u2500 book_props.npy\n\nYou can create multiple data-import tasks as follows\n\ntask_1 = utility.do_bulk_insert(\n    collection_name=\"book\",\n    files=[\"task_1/book_id.npy\", \"task_1/word_count.npy\", \"task_1/book_intro.npy\", \"task_1/book_props.npy\"]\n)\ntask_2 = utility.do_bulk_insert(\n    collection_name=\"book\",\n    files=[\"task_2/book_id.npy\", \"task_2/word_count.npy\", \"task_2/book_intro.npy\", \"task_2/book_props.npy\"]\n)\n\n\nCheck data searchability\nAfter a data-import task is complete, Milvus persists the imported data into segments and sends these segments to the index nodes for index-building. During the index-building process, these segments are unavailable for searches. Once such a process is complete, you need to call the load API again to load these segments into the query nodes. These segments will then be ready for searches.\n\nCheck the index-building progress\n\nPyMilvus provides a utility method to wait for the index-building process to complete.\n\nutility.wait_for_index_building_complete(collection_name)\n\n\nIn other SDKs, you can use the describe-index API to check the index-building progress.\n\nwhile (true) {\n    R<DescribeIndexResponse> response = milvusClient.describeIndex(\n        DescribeIndexParam.newBuilder()\n            .withCollectionName(collection_name)\n            .withIndexName(index_name)\n            .build());\n    IndexDescription desc = response.getData().getIndexDescriptions(0);\n    if (desc.getIndexedRows() == desc.getTotalRows()) {\n        break;\n    }\n}\n\n\n\nLoad new segments into query nodes\n\nNewly indexed segments need to be loaded manually as follows:\n\n  Python \n  Java\n\ncollection.load(_refresh = True)\n\nR<RpcStatus> response = milvusClient.loadCollection(\n    LoadCollectionParam.newBuilder()\n        .withCollectionName(collection_name)\n        .withRefresh(Boolean.TRUE)\n        .build());\n\n\n  \nThe _refresh parameter is False by default. Do not set it to True when you load a collection for the first time.\n  \n\n\n  \nThe withRefresh() method is optional. Do not call it with a Boolean.TRUE when you load a collection for the first time.\n  \n\nWhat's next\nLearn more basic operations of Milvus:\n\nBuild an index for vectors\nConduct a vector search\nConduct a hybrid search\nbulk_insert.md was last updated at 2023-07-24 00:03:45: fix: \"_refresh\" boolean parameter value typos in pythonEdit this pageReport a bugRequest doc changesOn this pageInsert Entities from FilesPrepare the data fileInsert entities from filesList tasksLimitsReferenceWhat's nextResourcesDocsBlogManaged serviceTutorialsBootcampDemoVideoToolsAttuMilvus CLISizing ToolMivlus backup ToolCommunityGet involvedSlackGithubForumMilvus. 2023 All rights reserved./*<![CDATA[*/window.pagePath=\"/docs/bulk_insert.md\";/*]]>*//*<![CDATA[*/window.___chunkMapping={\"polyfill\":[\"/polyfill.js\"],\"app\":[\"/app.js\"],\"component---src-pages-404-js\":[\"/component---src-pages-404-js.js\",\"/component---src-pages-404-js.ffef32ceb6972d1b6709.css\"],\"component---src-pages-bootcamp-jsx\":[\"/component---src-pages-bootcamp-jsx.js\",\"/component---src-pages-bootcamp-jsx.edf1a55d3c49094e9b41.css\"],\"component---src-pages-community-index-jsx\":[\"/component---src-pages-community-index-jsx.js\",\"/component---src-pages-community-index-jsx.c505d2a5fff7c1e4b2c8.css\"],\"component---src-pages-index-jsx\":[\"/component---src-pages-index-jsx.js\",\"/component---src-pages-index-jsx.45df0d4bdfe2db469d6c.css\"],\"component---src-pages-milvus-demos-index-jsx\":[\"/component---src-pages-milvus-demos-index-jsx.js\",\"/component---src-pages-milvus-demos-index-jsx.1a3d1c4caf417de255ae.css\"],\"component---src-pages-milvus-demos-reverse-image-search-jsx\":[\"/component---src-pages-milvus-demos-reverse-image-search-jsx.js\",\"/component---src-pages-milvus-demos-reverse-image-search-jsx.c41e4c5ead83649937e8.css\"],\"component---src-pages-slack-jsx\":[\"/component---src-pages-slack-jsx.js\",\"/component---src-pages-slack-jsx.b3b356986d01fe89f9d1.css\"],\"component---src-pages-tools-sizing-jsx\":[\"/component---src-pages-tools-sizing-jsx.js\",\"/component---src-pages-tools-sizing-jsx.66d1c05015f9508df90b.css\"],\"component---src-templates-api-doc-template-jsx\":[\"/component---src-templates-api-doc-template-jsx.js\",\"/component---src-templates-api-doc-template-jsx.6c57c73b99682695a5ef.css\"],\"component---src-templates-blog-list-template-jsx\":[\"/component---src-templates-blog-list-template-jsx.js\",\"/component---src-templates-blog-list-template-jsx.71f592374d478c258caf.css\"],\"component---src-templates-blog-template-jsx\":[\"/component---src-templates-blog-template-jsx.js\",\"/component---src-templates-blog-template-jsx.5068317f62135173b013.css\"],\"component---src-templates-doc-template-jsx\":[\"/component---src-templates-doc-template-jsx.js\",\"/component---src-templates-doc-template-jsx.0e85fbce360982ff881c.css\"]};/*]]>*/", "content_type": "text", "score": null, "meta": {"url": "https://milvus.io/docs/bulk_insert.md"}, "id_hash_keys": ["content"], "embedding": null, "id": "44f4208579a2708509d4e604aa3c189c"}